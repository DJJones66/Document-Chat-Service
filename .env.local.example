# .env.example

###########################################################
# ðŸš€ Chat with your Documents App Configuration
###########################################################

# --- General Settings ---
# Set to 'true' for a more detailed log output, useful for debugging.
# DEBUG=False
# The host and port for the FastAPI application.
# API_HOST=0.0.0.0
# API_PORT=8000
# CORS defaults to wide-open (origin reflection with credentials) for local/LAN testing; set to 0 to restrict.
CORS_ALLOW_ANY=1
# UI base URL (auto-added to CORS). Example for local UI on Vite or similar:
# UI_ORIGIN=http://localhost:5273
# Optional: override CORS when restricting
# CORS_ORIGINS=http://localhost:5173,http://localhost:5273
# CORS_ORIGIN_REGEX=https?://(localhost|127\.0\.0\.1|10\.\d+\.\d+\.\d+|192\.168\.\d+\.\d+|172\.(1[6-9]|2\d|3[0-1])\.\d+\.\d+)(:\\d+)?$

# --- LLM and Embedding Providers ---
# Choose your LLM provider. Supported: openai, ollama, groq, openrouter
LLM_PROVIDER=ollama
# Choose your Embedding provider. Supported: openai, ollama
EMBEDDING_PROVIDER=ollama

# --- Ollama Configuration (if selected) ---
# Base URL for your Ollama server.
# Set to http://localhost:11434 if running locally.
OLLAMA_LLM_BASE_URL=http://localhost:11434
# The Ollama model to use for chat.
OLLAMA_LLM_MODEL=llama3.1:8b

OLLAMA_EMBEDDING_BASE_URL=http://localhost:11434
# The Ollama model to use for embeddings.
OLLAMA_EMBEDDING_MODEL=mxbai-embed-large

# EMBEDDING OPTIMIZATION
# You can tune these based on your hardware.
# For 16GB RAM: batch 8, concurrency 2 is usually safe.
# For 8GB RAM: batch 4, concurrency 1 is safer.
EMBEDDING_BATCH_SIZE=8
EMBEDDING_CONCURRENCY=2
EMBEDDING_TIMEOUT=120
EMBEDDING_MAX_RETRIES=3
EMBEDDING_RETRY_DELAY=2.0
EMBEDDING_MAX_TOKENS=480

# --- Contextual Retrieval (Advanced) ---
# If enabled, uses a separate, smaller LLM to generate chunk context
ENABLE_CONTEXTUAL_RETRIEVAL=true
OLLAMA_CONTEXTUAL_LLM_BASE_URL=http://localhost:11434
OLLAMA_CONTEXTUAL_LLM_MODEL=llama3.2:3b

# Reduce contextual processing load
CONTEXTUAL_BATCH_SIZE=3
CONTEXTUAL_CHUNK_TIMEOUT=60
CONTEXTUAL_DOC_MAX_LENGTH=4000

# --- Groq Configuration (if selected) ---
# Your Groq API key.
# GROQ_API_KEY=""
# The Groq model to use.
# GROQ_LLM_MODEL="llama3-70b-8192"

# --- OpenAI Configuration (for Evaluation) ---
# Your OpenAI API key for evaluation judge service.
# OPENAI_EVALUATION_API_KEY=""
# The OpenAI model to use as judge for evaluation.
# OPENAI_EVALUATION_MODEL="gpt-5-mini"
# Timeout for evaluation judge API calls in seconds.
# OPENAI_EVALUATION_TIMEOUT=60

# --- Evaluation Settings ---
# Whether to initialize evaluation test collection on startup (default: false).
# Evaluation services (judge, repo) always initialize.
# INITIALIZE_TEST_COLLECTION=false
# Fixed ID for the evaluation test collection.
# EVALUATION_TEST_COLLECTION_ID="eval-test-collection-00000000-0000-0000-0000-000000000001"
# Name of the evaluation test collection.
# EVALUATION_TEST_COLLECTION_NAME="evaluation_test_collection"
# Directory containing evaluation test documents.
# EVALUATION_TEST_DOCS_DIR="evaluation_test_docs"

# --- Evaluation Performance ---
# Number of parallel context retrieval requests during evaluation (1-8).
# Tune based on your hardware:
# - Basic laptop (8GB RAM): 1
# - Mid-range PC (16GB RAM): 2
# - Gaming PC (32GB RAM): 4
# - Workstation (64GB+ RAM): 8
# See docs/OLLAMA_PERFORMANCE_OPTIMIZATION.md for detailed guide.
EVALUATION_CONCURRENCY=2

# --- RAG Optimization ---
# Context window management for optimal retrieval
# The system automatically detects model context windows from Ollama.
# These settings control how chunks are retrieved based on available token budget.

# Default context window when detection fails (conservative default)
# DEFAULT_CONTEXT_WINDOW=4096

# Safety margin: use only this fraction of context window (75% = 0.75)
# Leaves room for generation and prevents token truncation
# CONTEXT_SAFETY_MARGIN=0.75

# Reverse context for Ollama (highly recommended)
# Places most relevant chunks LAST since Ollama strips tokens from TOP
# REVERSE_CONTEXT_FOR_OLLAMA=true

# Min/max number of chunks to retrieve (dynamic based on model)
# MIN_TOP_K=2
# MAX_TOP_K=10

# Expected average tokens per chunk for budget calculation
# AVG_CHUNK_TOKENS=200

# --- Database & Storage Paths ---
# Directory for persistent data, including the database and files.
# UPLOADS_DIR=./data/uploads
# CHROMA_PERSIST_DIR=./data/vector_db
# BM25_PERSIST_DIR=./data/bm25_index
# SQLite database file path.
# DATABASE_URL="sqlite+aiosqlite:///./data/app.db"
# Example for PostgreSQL:
# DATABASE_URL="postgresql://user:password@host:port/dbname"

# --- Document Processing ---
# URL for the Document Processor microservice (BrainDrive-Document-AI).
# DOCUMENT_PROCESSOR_API_URL="https://your-document-processor.com"
DOCUMENT_PROCESSOR_API_URL="http://localhost:18080/documents/"
DOCUMENT_PROCESSOR_API_KEY=""
DOCUMENT_PROCESSOR_TIMEOUT=300
DOCUMENT_PROCESSOR_MAX_RETRIES=3

# Docker build
DOCKER_BUILDKIT=1
COMPOSE_DOCKER_CLI_BUILD=1

# Chroma DB
CHROMA_TELEMETRY=0

# DO NOT TRACK
SCARF_NO_ANALYTICS=true
DO_NOT_TRACK=true
ANONYMIZED_TELEMETRY=false
